{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The goal of this project is to create a data mart for the massive Travel data that serves as a \"golden record\" for reporting and analytics purposes. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import date\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Configuring the AWS Credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dw.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['CREDENTIALS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['CREDENTIALS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "I am going to load the data into the workspace, examine it, clean the datasets, join them where necessary, apply transformations, design a star schema for data mart, load the data into the data mart.\n",
    "I used US Airport codes, US Travel data, US Cities demographics data. My end solution is a data mart. I used Apache Spark  on Amazon S3\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "I used:\n",
    "US Travel Data:\n",
    "This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.\n",
    "\n",
    "US Cities Demographics Data: This data set contains demographics information, average size of household for major cities and their respective states. This data comes from OpenSoft. You can read more about it here.\n",
    "\n",
    "US Airport Codes Data: This is a simple table of airport codes and corresponding cities. It comes from here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://spark-packages.org/package/saurfang/spark-sas7bdat\n",
    "# Initializing a Spark Session\n",
    "spark = SparkSession.builder\\\n",
    "            .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.3\")\\\n",
    "            .enableHiveSupport()\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Loading the US Travel data that is in sas7bdat format\n",
    "i94_df = spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "                .option('header','True')\\\n",
    "                .load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Loading the cities demographics data\n",
    "cities_df = spark.read.format('csv')\\\n",
    "                    .option('header','True')\\\n",
    "                    .load('us-cities-demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Loading the airport codes data\n",
    "airport_df = spark.read.format('csv')\\\n",
    "                    .option('header','True')\\\n",
    "                    .load('airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Step 2: Explore and Assess the Data\n",
    "\n",
    "Explore the Data\n",
    "\n",
    "Missing values,\n",
    "unclean data (entire dataset is in one column blob) that needs to be transformed,\n",
    "SAS Date format on dates\n",
    "\n",
    "Cleaning Steps\n",
    "\n",
    "Created a spark UDF to transform the arrival date\n",
    "Cleaned up for missing values and duplications\n",
    "Transformed a single column blob data to multi column data that can be resuable for reporting and analytics\n",
    "Created a new data set based on US Cities and Demographics that would work better with my data mart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cicid [Row(cicid_missing=None)]\n",
      "i94yr [Row(i94yr_missing=None)]\n",
      "i94mon [Row(i94mon_missing=None)]\n",
      "i94cit [Row(i94cit_missing=None)]\n",
      "i94res [Row(i94res_missing=None)]\n",
      "i94port [Row(i94port_missing=None)]\n",
      "i94addr [Row(i94addr_missing=152592)]\n",
      "depdate [Row(depdate_missing=142457)]\n",
      "i94bir [Row(i94bir_missing=802)]\n",
      "i94visa [Row(i94visa_missing=None)]\n",
      "count [Row(count_missing=None)]\n",
      "dtadfile [Row(dtadfile_missing=1)]\n",
      "visapost [Row(visapost_missing=1881250)]\n",
      "occup [Row(occup_missing=3088187)]\n",
      "entdepa [Row(entdepa_missing=238)]\n",
      "entdepd [Row(entdepd_missing=138429)]\n",
      "entdepu [Row(entdepu_missing=3095921)]\n",
      "matflag [Row(matflag_missing=138429)]\n",
      "biryear [Row(biryear_missing=802)]\n",
      "dtaddto [Row(dtaddto_missing=477)]\n",
      "gender [Row(gender_missing=414269)]\n",
      "insnum [Row(insnum_missing=2982605)]\n",
      "airline [Row(airline_missing=83627)]\n",
      "admnum [Row(admnum_missing=None)]\n",
      "fltno [Row(fltno_missing=19549)]\n",
      "visatype [Row(visatype_missing=None)]\n"
     ]
    }
   ],
   "source": [
    "# missing values estimation\n",
    "columnlist = i94_df.columns\n",
    "\n",
    "for i in columnlist:\n",
    "    a = spark.sql(\"select sum(case when {} is null then 1 else 0 end) as {}_missing from i94_df \\\n",
    "                where {} is null\".format(i,i,i)).collect()\n",
    "    print(i,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Deduplication process on selected columns\n",
    "# Data Cleaning\n",
    "i94_df_clean = i94_df.dropna(how = \"any\", subset = [\"i94addr\", \"depdate\"])\n",
    "airport_df_clean = airport_df.dropna(how = \"any\", subset = [\"iso_region\", \"local_code\",\"StateCode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>StateCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KSTP</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>St Paul Downtown Holman Field</td>\n",
       "      <td>705</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-MN</td>\n",
       "      <td>St Paul</td>\n",
       "      <td>KSTP</td>\n",
       "      <td>STP</td>\n",
       "      <td>STP</td>\n",
       "      <td>-93.05999755859375, 44.93450164794922</td>\n",
       "      <td>MN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident            type                           name elevation_ft continent  \\\n",
       "0  KSTP  medium_airport  St Paul Downtown Holman Field          705        NA   \n",
       "\n",
       "  iso_country iso_region municipality gps_code iata_code local_code  \\\n",
       "0          US      US-MN      St Paul     KSTP       STP        STP   \n",
       "\n",
       "                             coordinates StateCode  \n",
       "0  -93.05999755859375, 44.93450164794922        MN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining the data \n",
    "#airport_df.groupBy(\"continent\").agg(countDistinct(\"name\")).toPandas()\n",
    "airport_df.filter(airport_df.local_code == \"STP\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|       admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null|1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null| 3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS| 6.66643185E8|   93|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Transforming a single column blob to multi column data set\n",
    "\n",
    "split_col = split(cities_df['City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count'], ';')\n",
    "cities_df = cities_df.withColumn('City', split_col.getItem(0))\n",
    "cities_df = cities_df.withColumn('State', split_col.getItem(1))\n",
    "cities_df = cities_df.withColumn('MedianAge', split_col.getItem(2))\n",
    "cities_df = cities_df.withColumn('MalePop', split_col.getItem(3))\n",
    "cities_df = cities_df.withColumn('FemalePop', split_col.getItem(4))\n",
    "cities_df = cities_df.withColumn('TotalPop', split_col.getItem(5))\n",
    "cities_df = cities_df.withColumn('NumberofVeterans', split_col.getItem(6))\n",
    "cities_df = cities_df.withColumn('ForeignBorn', split_col.getItem(7))\n",
    "cities_df = cities_df.withColumn('AvgHouseholdSize', split_col.getItem(8))\n",
    "cities_df = cities_df.withColumn('StateCode', split_col.getItem(9))\n",
    "cities_df = cities_df.withColumn('Race', split_col.getItem(10))\n",
    "cities_df = cities_df.withColumn('Count', split_col.getItem(11))\n",
    "#df_states.withColumn(\"col1\", split(col(\"State_Name\"), \"-\")\n",
    "#                     .getItem(0))\n",
    "cities_df_clean = cities_df.drop('City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count')\n",
    "\n",
    "#cities_df_clean.toPandas()\n",
    "airport_df = airport_df.withColumn(\"StateCode\", split(col(\"iso_region\"),\"-\").getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "states_df = cities_df_clean.groupBy(\"State\",\"StateCode\") \\\n",
    "    .agg(sum(\"MalePop\").alias(\"Male\"), \\\n",
    "         sum(\"FemalePop\").alias(\"Female\"), \\\n",
    "         sum(\"TotalPop\").alias(\"Population\"), \\\n",
    "         sum(\"NumberofVeterans\").alias(\"Veterans\"), \\\n",
    "         sum(\"ForeignBorn\").alias(\"AlienPopulation\"), \\\n",
    "         avg(\"AvgHouseholdSize\").alias(\"AvgHouseHoldSize\"), \\\n",
    "         avg(\"MedianAge\").alias(\"AverageAge\") \n",
    "     ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>StateCode</th>\n",
       "      <th>Male</th>\n",
       "      <th>Female</th>\n",
       "      <th>Population</th>\n",
       "      <th>Veterans</th>\n",
       "      <th>AlienPopulation</th>\n",
       "      <th>AvgHouseHoldSize</th>\n",
       "      <th>AverageAge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>MS</td>\n",
       "      <td>527627.0</td>\n",
       "      <td>613916.0</td>\n",
       "      <td>1141543.0</td>\n",
       "      <td>67314.0</td>\n",
       "      <td>21233.0</td>\n",
       "      <td>2.601111</td>\n",
       "      <td>33.211111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Utah</td>\n",
       "      <td>UT</td>\n",
       "      <td>2586752.0</td>\n",
       "      <td>2532925.0</td>\n",
       "      <td>5119677.0</td>\n",
       "      <td>193165.0</td>\n",
       "      <td>651811.0</td>\n",
       "      <td>3.156875</td>\n",
       "      <td>30.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>SD</td>\n",
       "      <td>613590.0</td>\n",
       "      <td>611900.0</td>\n",
       "      <td>1225490.0</td>\n",
       "      <td>80435.0</td>\n",
       "      <td>76545.0</td>\n",
       "      <td>2.345000</td>\n",
       "      <td>37.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>KY</td>\n",
       "      <td>2262415.0</td>\n",
       "      <td>2386970.0</td>\n",
       "      <td>4649385.0</td>\n",
       "      <td>280125.0</td>\n",
       "      <td>332440.0</td>\n",
       "      <td>2.395000</td>\n",
       "      <td>35.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>CA</td>\n",
       "      <td>61055672.0</td>\n",
       "      <td>62388681.0</td>\n",
       "      <td>123444353.0</td>\n",
       "      <td>4617022.0</td>\n",
       "      <td>37059662.0</td>\n",
       "      <td>3.095325</td>\n",
       "      <td>36.173964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nebraska</td>\n",
       "      <td>NE</td>\n",
       "      <td>1786665.0</td>\n",
       "      <td>1819500.0</td>\n",
       "      <td>3606165.0</td>\n",
       "      <td>195985.0</td>\n",
       "      <td>356105.0</td>\n",
       "      <td>2.435000</td>\n",
       "      <td>33.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>NH</td>\n",
       "      <td>488855.0</td>\n",
       "      <td>502135.0</td>\n",
       "      <td>990990.0</td>\n",
       "      <td>55025.0</td>\n",
       "      <td>135995.0</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>37.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>DE</td>\n",
       "      <td>163400.0</td>\n",
       "      <td>196385.0</td>\n",
       "      <td>359785.0</td>\n",
       "      <td>15315.0</td>\n",
       "      <td>16680.0</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>36.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>MN</td>\n",
       "      <td>3478803.0</td>\n",
       "      <td>3565362.0</td>\n",
       "      <td>7044165.0</td>\n",
       "      <td>321738.0</td>\n",
       "      <td>1069888.0</td>\n",
       "      <td>2.496852</td>\n",
       "      <td>35.579630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>NC</td>\n",
       "      <td>7330525.0</td>\n",
       "      <td>7970470.0</td>\n",
       "      <td>15300995.0</td>\n",
       "      <td>830730.0</td>\n",
       "      <td>1896635.0</td>\n",
       "      <td>2.475000</td>\n",
       "      <td>33.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>NV</td>\n",
       "      <td>5593225.0</td>\n",
       "      <td>5610495.0</td>\n",
       "      <td>11203720.0</td>\n",
       "      <td>760235.0</td>\n",
       "      <td>2406685.0</td>\n",
       "      <td>2.813333</td>\n",
       "      <td>36.088889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Washington</td>\n",
       "      <td>WA</td>\n",
       "      <td>6228025.0</td>\n",
       "      <td>6272510.0</td>\n",
       "      <td>12500535.0</td>\n",
       "      <td>765630.0</td>\n",
       "      <td>2204810.0</td>\n",
       "      <td>2.597647</td>\n",
       "      <td>35.288235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>LA</td>\n",
       "      <td>3134990.0</td>\n",
       "      <td>3367985.0</td>\n",
       "      <td>6502975.0</td>\n",
       "      <td>348855.0</td>\n",
       "      <td>417095.0</td>\n",
       "      <td>2.465000</td>\n",
       "      <td>34.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>ID</td>\n",
       "      <td>995515.0</td>\n",
       "      <td>998900.0</td>\n",
       "      <td>1994415.0</td>\n",
       "      <td>131900.0</td>\n",
       "      <td>140630.0</td>\n",
       "      <td>2.710000</td>\n",
       "      <td>34.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>NM</td>\n",
       "      <td>2045050.0</td>\n",
       "      <td>2150160.0</td>\n",
       "      <td>4195210.0</td>\n",
       "      <td>302370.0</td>\n",
       "      <td>445560.0</td>\n",
       "      <td>2.585000</td>\n",
       "      <td>37.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Maine</td>\n",
       "      <td>ME</td>\n",
       "      <td>157400.0</td>\n",
       "      <td>176960.0</td>\n",
       "      <td>334360.0</td>\n",
       "      <td>18330.0</td>\n",
       "      <td>46145.0</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>40.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>TN</td>\n",
       "      <td>5124189.0</td>\n",
       "      <td>5565976.0</td>\n",
       "      <td>10690165.0</td>\n",
       "      <td>586202.0</td>\n",
       "      <td>900149.0</td>\n",
       "      <td>2.462955</td>\n",
       "      <td>34.311364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>CO</td>\n",
       "      <td>7273095.0</td>\n",
       "      <td>7405250.0</td>\n",
       "      <td>14678345.0</td>\n",
       "      <td>939480.0</td>\n",
       "      <td>1688155.0</td>\n",
       "      <td>2.560000</td>\n",
       "      <td>35.818750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>AZ</td>\n",
       "      <td>11137275.0</td>\n",
       "      <td>11360435.0</td>\n",
       "      <td>22497710.0</td>\n",
       "      <td>1322525.0</td>\n",
       "      <td>3411565.0</td>\n",
       "      <td>2.774375</td>\n",
       "      <td>35.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>PR</td>\n",
       "      <td>930052.0</td>\n",
       "      <td>1100795.0</td>\n",
       "      <td>2030847.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>WI</td>\n",
       "      <td>3444015.0</td>\n",
       "      <td>3621710.0</td>\n",
       "      <td>7065725.0</td>\n",
       "      <td>305670.0</td>\n",
       "      <td>623435.0</td>\n",
       "      <td>2.417778</td>\n",
       "      <td>33.511111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "      <td>2448200.0</td>\n",
       "      <td>2715106.0</td>\n",
       "      <td>5163306.0</td>\n",
       "      <td>352896.0</td>\n",
       "      <td>252541.0</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>36.161765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>OR</td>\n",
       "      <td>3537215.0</td>\n",
       "      <td>3645330.0</td>\n",
       "      <td>7182545.0</td>\n",
       "      <td>394740.0</td>\n",
       "      <td>928765.0</td>\n",
       "      <td>2.538750</td>\n",
       "      <td>36.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>KS</td>\n",
       "      <td>2820725.0</td>\n",
       "      <td>2920645.0</td>\n",
       "      <td>5741370.0</td>\n",
       "      <td>323945.0</td>\n",
       "      <td>593225.0</td>\n",
       "      <td>2.591429</td>\n",
       "      <td>34.828571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Florida</td>\n",
       "      <td>FL</td>\n",
       "      <td>15461937.0</td>\n",
       "      <td>16626425.0</td>\n",
       "      <td>32306132.0</td>\n",
       "      <td>1861951.0</td>\n",
       "      <td>7845566.0</td>\n",
       "      <td>2.760274</td>\n",
       "      <td>39.528829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>PA</td>\n",
       "      <td>5514704.0</td>\n",
       "      <td>5988097.0</td>\n",
       "      <td>11502801.0</td>\n",
       "      <td>514487.0</td>\n",
       "      <td>1439936.0</td>\n",
       "      <td>2.507576</td>\n",
       "      <td>33.951515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>ND</td>\n",
       "      <td>476175.0</td>\n",
       "      <td>471275.0</td>\n",
       "      <td>947450.0</td>\n",
       "      <td>51495.0</td>\n",
       "      <td>57460.0</td>\n",
       "      <td>2.145000</td>\n",
       "      <td>34.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>23422799.0</td>\n",
       "      <td>25579256.0</td>\n",
       "      <td>49002055.0</td>\n",
       "      <td>1019097.0</td>\n",
       "      <td>17186873.0</td>\n",
       "      <td>2.770370</td>\n",
       "      <td>35.570370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>SC</td>\n",
       "      <td>1265291.0</td>\n",
       "      <td>1321685.0</td>\n",
       "      <td>2586976.0</td>\n",
       "      <td>163334.0</td>\n",
       "      <td>134019.0</td>\n",
       "      <td>2.469583</td>\n",
       "      <td>33.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Texas</td>\n",
       "      <td>TX</td>\n",
       "      <td>34862194.0</td>\n",
       "      <td>35691659.0</td>\n",
       "      <td>70553853.0</td>\n",
       "      <td>3429512.0</td>\n",
       "      <td>14498054.0</td>\n",
       "      <td>2.845128</td>\n",
       "      <td>33.379487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>OH</td>\n",
       "      <td>5853254.0</td>\n",
       "      <td>6243296.0</td>\n",
       "      <td>12096550.0</td>\n",
       "      <td>633456.0</td>\n",
       "      <td>873891.0</td>\n",
       "      <td>2.298571</td>\n",
       "      <td>35.593878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>DC</td>\n",
       "      <td>1598525.0</td>\n",
       "      <td>1762615.0</td>\n",
       "      <td>3361140.0</td>\n",
       "      <td>129815.0</td>\n",
       "      <td>475585.0</td>\n",
       "      <td>2.240000</td>\n",
       "      <td>33.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>IA</td>\n",
       "      <td>1772066.0</td>\n",
       "      <td>1831937.0</td>\n",
       "      <td>3604003.0</td>\n",
       "      <td>197225.0</td>\n",
       "      <td>309829.0</td>\n",
       "      <td>2.382059</td>\n",
       "      <td>32.544118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>NJ</td>\n",
       "      <td>3423033.0</td>\n",
       "      <td>3507991.0</td>\n",
       "      <td>6931024.0</td>\n",
       "      <td>146632.0</td>\n",
       "      <td>2327750.0</td>\n",
       "      <td>2.960877</td>\n",
       "      <td>35.254386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Montana</td>\n",
       "      <td>MT</td>\n",
       "      <td>438535.0</td>\n",
       "      <td>467935.0</td>\n",
       "      <td>906470.0</td>\n",
       "      <td>69270.0</td>\n",
       "      <td>29885.0</td>\n",
       "      <td>2.275000</td>\n",
       "      <td>35.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>IN</td>\n",
       "      <td>4399882.0</td>\n",
       "      <td>4697912.0</td>\n",
       "      <td>9097794.0</td>\n",
       "      <td>447620.0</td>\n",
       "      <td>713623.0</td>\n",
       "      <td>2.478431</td>\n",
       "      <td>33.827451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>HI</td>\n",
       "      <td>884035.0</td>\n",
       "      <td>879795.0</td>\n",
       "      <td>1763830.0</td>\n",
       "      <td>116065.0</td>\n",
       "      <td>506560.0</td>\n",
       "      <td>2.690000</td>\n",
       "      <td>41.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>RI</td>\n",
       "      <td>974791.0</td>\n",
       "      <td>1011321.0</td>\n",
       "      <td>1986112.0</td>\n",
       "      <td>87236.0</td>\n",
       "      <td>431507.0</td>\n",
       "      <td>2.528947</td>\n",
       "      <td>37.652632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>MO</td>\n",
       "      <td>3666310.0</td>\n",
       "      <td>3929660.0</td>\n",
       "      <td>7595970.0</td>\n",
       "      <td>457265.0</td>\n",
       "      <td>495475.0</td>\n",
       "      <td>2.410000</td>\n",
       "      <td>34.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>AR</td>\n",
       "      <td>1400724.0</td>\n",
       "      <td>1482165.0</td>\n",
       "      <td>2882889.0</td>\n",
       "      <td>154390.0</td>\n",
       "      <td>307753.0</td>\n",
       "      <td>2.526897</td>\n",
       "      <td>32.737931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>MI</td>\n",
       "      <td>5217245.0</td>\n",
       "      <td>5667993.0</td>\n",
       "      <td>10885238.0</td>\n",
       "      <td>490435.0</td>\n",
       "      <td>1214547.0</td>\n",
       "      <td>2.505570</td>\n",
       "      <td>37.011392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>GA</td>\n",
       "      <td>4101605.0</td>\n",
       "      <td>4453555.0</td>\n",
       "      <td>8555160.0</td>\n",
       "      <td>522575.0</td>\n",
       "      <td>738925.0</td>\n",
       "      <td>2.552727</td>\n",
       "      <td>33.809091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>IL</td>\n",
       "      <td>10943864.0</td>\n",
       "      <td>11570526.0</td>\n",
       "      <td>22514390.0</td>\n",
       "      <td>723049.0</td>\n",
       "      <td>4632600.0</td>\n",
       "      <td>2.731868</td>\n",
       "      <td>35.708791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>MD</td>\n",
       "      <td>3139755.0</td>\n",
       "      <td>3420890.0</td>\n",
       "      <td>6560645.0</td>\n",
       "      <td>320715.0</td>\n",
       "      <td>1148970.0</td>\n",
       "      <td>2.655000</td>\n",
       "      <td>36.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>CT</td>\n",
       "      <td>2123435.0</td>\n",
       "      <td>2231661.0</td>\n",
       "      <td>4355096.0</td>\n",
       "      <td>122546.0</td>\n",
       "      <td>1114250.0</td>\n",
       "      <td>2.666154</td>\n",
       "      <td>35.002564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>AK</td>\n",
       "      <td>764725.0</td>\n",
       "      <td>728750.0</td>\n",
       "      <td>1493475.0</td>\n",
       "      <td>137460.0</td>\n",
       "      <td>166290.0</td>\n",
       "      <td>2.770000</td>\n",
       "      <td>32.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>MA</td>\n",
       "      <td>4841101.0</td>\n",
       "      <td>5155944.0</td>\n",
       "      <td>9997045.0</td>\n",
       "      <td>329190.0</td>\n",
       "      <td>2573815.0</td>\n",
       "      <td>2.559855</td>\n",
       "      <td>35.546377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>OK</td>\n",
       "      <td>3572865.0</td>\n",
       "      <td>3672110.0</td>\n",
       "      <td>7244975.0</td>\n",
       "      <td>477340.0</td>\n",
       "      <td>755870.0</td>\n",
       "      <td>2.598333</td>\n",
       "      <td>33.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>VA</td>\n",
       "      <td>5802370.0</td>\n",
       "      <td>6015740.0</td>\n",
       "      <td>11818110.0</td>\n",
       "      <td>1148830.0</td>\n",
       "      <td>1346270.0</td>\n",
       "      <td>2.584286</td>\n",
       "      <td>34.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   State StateCode        Male      Female   Population  \\\n",
       "0            Mississippi        MS    527627.0    613916.0    1141543.0   \n",
       "1                   Utah        UT   2586752.0   2532925.0    5119677.0   \n",
       "2           South Dakota        SD    613590.0    611900.0    1225490.0   \n",
       "3               Kentucky        KY   2262415.0   2386970.0    4649385.0   \n",
       "4             California        CA  61055672.0  62388681.0  123444353.0   \n",
       "5               Nebraska        NE   1786665.0   1819500.0    3606165.0   \n",
       "6          New Hampshire        NH    488855.0    502135.0     990990.0   \n",
       "7               Delaware        DE    163400.0    196385.0     359785.0   \n",
       "8              Minnesota        MN   3478803.0   3565362.0    7044165.0   \n",
       "9         North Carolina        NC   7330525.0   7970470.0   15300995.0   \n",
       "10                Nevada        NV   5593225.0   5610495.0   11203720.0   \n",
       "11            Washington        WA   6228025.0   6272510.0   12500535.0   \n",
       "12             Louisiana        LA   3134990.0   3367985.0    6502975.0   \n",
       "13                 Idaho        ID    995515.0    998900.0    1994415.0   \n",
       "14            New Mexico        NM   2045050.0   2150160.0    4195210.0   \n",
       "15                 Maine        ME    157400.0    176960.0     334360.0   \n",
       "16             Tennessee        TN   5124189.0   5565976.0   10690165.0   \n",
       "17              Colorado        CO   7273095.0   7405250.0   14678345.0   \n",
       "18               Arizona        AZ  11137275.0  11360435.0   22497710.0   \n",
       "19           Puerto Rico        PR    930052.0   1100795.0    2030847.0   \n",
       "20             Wisconsin        WI   3444015.0   3621710.0    7065725.0   \n",
       "21               Alabama        AL   2448200.0   2715106.0    5163306.0   \n",
       "22                Oregon        OR   3537215.0   3645330.0    7182545.0   \n",
       "23                Kansas        KS   2820725.0   2920645.0    5741370.0   \n",
       "24               Florida        FL  15461937.0  16626425.0   32306132.0   \n",
       "25          Pennsylvania        PA   5514704.0   5988097.0   11502801.0   \n",
       "26          North Dakota        ND    476175.0    471275.0     947450.0   \n",
       "27              New York        NY  23422799.0  25579256.0   49002055.0   \n",
       "28        South Carolina        SC   1265291.0   1321685.0    2586976.0   \n",
       "29                 Texas        TX  34862194.0  35691659.0   70553853.0   \n",
       "30                  Ohio        OH   5853254.0   6243296.0   12096550.0   \n",
       "31  District of Columbia        DC   1598525.0   1762615.0    3361140.0   \n",
       "32                  Iowa        IA   1772066.0   1831937.0    3604003.0   \n",
       "33            New Jersey        NJ   3423033.0   3507991.0    6931024.0   \n",
       "34               Montana        MT    438535.0    467935.0     906470.0   \n",
       "35               Indiana        IN   4399882.0   4697912.0    9097794.0   \n",
       "36                Hawaii        HI    884035.0    879795.0    1763830.0   \n",
       "37          Rhode Island        RI    974791.0   1011321.0    1986112.0   \n",
       "38              Missouri        MO   3666310.0   3929660.0    7595970.0   \n",
       "39              Arkansas        AR   1400724.0   1482165.0    2882889.0   \n",
       "40              Michigan        MI   5217245.0   5667993.0   10885238.0   \n",
       "41               Georgia        GA   4101605.0   4453555.0    8555160.0   \n",
       "42              Illinois        IL  10943864.0  11570526.0   22514390.0   \n",
       "43              Maryland        MD   3139755.0   3420890.0    6560645.0   \n",
       "44           Connecticut        CT   2123435.0   2231661.0    4355096.0   \n",
       "45                Alaska        AK    764725.0    728750.0    1493475.0   \n",
       "46         Massachusetts        MA   4841101.0   5155944.0    9997045.0   \n",
       "47              Oklahoma        OK   3572865.0   3672110.0    7244975.0   \n",
       "48              Virginia        VA   5802370.0   6015740.0   11818110.0   \n",
       "\n",
       "     Veterans  AlienPopulation  AvgHouseHoldSize  AverageAge  \n",
       "0     67314.0          21233.0          2.601111   33.211111  \n",
       "1    193165.0         651811.0          3.156875   30.862500  \n",
       "2     80435.0          76545.0          2.345000   37.050000  \n",
       "3    280125.0         332440.0          2.395000   35.950000  \n",
       "4   4617022.0       37059662.0          3.095325   36.173964  \n",
       "5    195985.0         356105.0          2.435000   33.250000  \n",
       "6     55025.0         135995.0          2.430000   37.800000  \n",
       "7     15315.0          16680.0          2.450000   36.400000  \n",
       "8    321738.0        1069888.0          2.496852   35.579630  \n",
       "9    830730.0        1896635.0          2.475000   33.785714  \n",
       "10   760235.0        2406685.0          2.813333   36.088889  \n",
       "11   765630.0        2204810.0          2.597647   35.288235  \n",
       "12   348855.0         417095.0          2.465000   34.625000  \n",
       "13   131900.0         140630.0          2.710000   34.766667  \n",
       "14   302370.0         445560.0          2.585000   37.775000  \n",
       "15    18330.0          46145.0          2.130000   40.300000  \n",
       "16   586202.0         900149.0          2.462955   34.311364  \n",
       "17   939480.0        1688155.0          2.560000   35.818750  \n",
       "18  1322525.0        3411565.0          2.774375   35.037500  \n",
       "19        NaN              NaN               NaN   40.730769  \n",
       "20   305670.0         623435.0          2.417778   33.511111  \n",
       "21   352896.0         252541.0          2.430000   36.161765  \n",
       "22   394740.0         928765.0          2.538750   36.125000  \n",
       "23   323945.0         593225.0          2.591429   34.828571  \n",
       "24  1861951.0        7845566.0          2.760274   39.528829  \n",
       "25   514487.0        1439936.0          2.507576   33.951515  \n",
       "26    51495.0          57460.0          2.145000   34.350000  \n",
       "27  1019097.0       17186873.0          2.770370   35.570370  \n",
       "28   163334.0         134019.0          2.469583   33.825000  \n",
       "29  3429512.0       14498054.0          2.845128   33.379487  \n",
       "30   633456.0         873891.0          2.298571   35.593878  \n",
       "31   129815.0         475585.0          2.240000   33.800000  \n",
       "32   197225.0         309829.0          2.382059   32.544118  \n",
       "33   146632.0        2327750.0          2.960877   35.254386  \n",
       "34    69270.0          29885.0          2.275000   35.500000  \n",
       "35   447620.0         713623.0          2.478431   33.827451  \n",
       "36   116065.0         506560.0          2.690000   41.400000  \n",
       "37    87236.0         431507.0          2.528947   37.652632  \n",
       "38   457265.0         495475.0          2.410000   34.866667  \n",
       "39   154390.0         307753.0          2.526897   32.737931  \n",
       "40   490435.0        1214547.0          2.505570   37.011392  \n",
       "41   522575.0         738925.0          2.552727   33.809091  \n",
       "42   723049.0        4632600.0          2.731868   35.708791  \n",
       "43   320715.0        1148970.0          2.655000   36.370000  \n",
       "44   122546.0        1114250.0          2.666154   35.002564  \n",
       "45   137460.0         166290.0          2.770000   32.200000  \n",
       "46   329190.0        2573815.0          2.559855   35.546377  \n",
       "47   477340.0         755870.0          2.598333   33.400000  \n",
       "48  1148830.0        1346270.0          2.584286   34.428571  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#airport_df.toPandas()\n",
    "states_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def arrival_date(arrdate):\n",
    "    if arrdate:\n",
    "        return (datetime(1960,1,1).date + timedelta(arrdate)).isoformat()\n",
    "    return None\n",
    "\n",
    "i94_df_clean = i94_df_clean.withColumn(\"arrdate\",arrival_date(i94_df_clean.arrdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# initialize spark sql\n",
    "i94_df_clean.createOrReplaceTempView(\"i94\")\n",
    "cities_df_clean.createOrReplaceTempView(\"cities\")\n",
    "airport_df_clean.createOrReplaceTempView(\"airport\")\n",
    "states_df.createOrReplaceTempView(\"states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.\\\n",
    "#config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "#.enableHiveSupport().getOrCreate()\n",
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The conceptual data model is desgined by including all three data sets for analysis and in depth reporting into travellers, airport informations, their respective state based information and other major information that would be appropritate for reporting and analytics\n",
    "\n",
    "##### 3.1.1 Conceptual Data Model along with Data Dictionary\n",
    "\n",
    "* Fact_i94 - fact table\n",
    "   * cicid: CIC_UniqueID\n",
    "   * i94cit: origin city code\n",
    "   * i94res: residential code\n",
    "   * i94port: port of entry \n",
    "   * i94addr: state code within united states, final state (different from port of entry state)\n",
    "   * visapost: visa issuance information\n",
    "   * admnum: admission number for that entry\n",
    "\n",
    "\n",
    "* DimAirport - Airport Dimension Table\n",
    "   * i94Port: PortOfEntry\n",
    "   * name: name of the airport \n",
    "   * coordinates: latitude and longtitude coordinates\n",
    "   * iso_region: region of the airport \n",
    "   * municipality: Municipality where the airport is located\n",
    "   * gps_code: GPS code of the airport\n",
    "   * type: type of airport whether it is small or large \n",
    "   \n",
    "* DimTraveller - Traveller Dimension Table\n",
    "   * cicid: CIC_UniqueID of the traveller\n",
    "   * gender: gender of the traveller\n",
    "   * biryear: birth year of the traveller\n",
    "   * visatype: visa type of the traveller\n",
    "   * occup: occupation of the traveller\n",
    "\n",
    "\n",
    "* DimTravelFlags - Various Flags used for the Travel\n",
    "   * cicid: CIC_UniqueID\n",
    "   * ENTDEPA: ArrivalFlag\n",
    "   * ENTDEPD: DepartureFlag\n",
    "   * ENTDEPU: UpdateFlag\n",
    "   * MATFLAG: MatchFlag\n",
    "\n",
    "* DimAirline - Airline that Traveller used to flyDimension Table\n",
    "   * cicid: CIC_UniqueID\n",
    "   * arrdate: ArrivalDate\n",
    "   * AIRLINE: Airline\n",
    "   * FLTNO: FlightNumber of that airline\n",
    "\n",
    "* DimTravelInfo - Dimension that contains travel information\n",
    "   * cicid as CIC_UniqueID\n",
    "   * VISATYPE: TypeOfVisa\n",
    "   * ADMNUM: AdmissionNumber\n",
    "   * VISAPOST: VisaIssuance\n",
    "   * I94VISA: PurposeOfTravel\n",
    "   * I94MODE: ModeOfTravel\n",
    "\n",
    "* DimStateInfo - Dimension about State demographic information\n",
    "   * State: State \n",
    "   * StateCode: Short code of every state\n",
    "   * Male: Male population in that state\n",
    "   * Female: Female population in that state\n",
    "   * Population: Total population in that state\n",
    "   * Veterans: Veteran population in that state\n",
    "   * AlienPopulation: Foreign born population in that state\n",
    "   * AvgHouseHoldSize: average size of a household in that state\n",
    "   * AverageAge: average age of a person in that state\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The following steps are taken to pipeline the raw data to data marts:\n",
    "fact and dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reading the parquet\n",
    "#filename = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark').load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# loading data into the data lake as a data mart\n",
    "\n",
    "fact_i94 = spark.sql(\"\"\"\n",
    "Select \n",
    "    cicid as CIC_UniqueID,\n",
    "    i94cit,\n",
    "    i94res,\n",
    "    i94port,\n",
    "    i94addr,\n",
    "    visapost,\n",
    "    admnum\n",
    "From \n",
    "    i94\n",
    "\"\"\").dropDuplicates()\n",
    "\n",
    "dimAirport = spark.sql(\"\"\"\n",
    "Select\n",
    "    a.i94Port as PortOfEntry,\n",
    "    b.name as AirportName,\n",
    "    b.coordinates as Coordinates,\n",
    "    b.iso_region as Region,\n",
    "    b.municipality as Municipality,\n",
    "    b.gps_code as GPSCode,\n",
    "    b.type as AirportType\n",
    "From\n",
    "    i94 a\n",
    "Join\n",
    "    airport b\n",
    "On\n",
    "    a.i94Port == b.local_code\n",
    "\"\"\").dropDuplicates()\n",
    "\n",
    "dimTraveller = spark.sql(\"\"\"\n",
    "Select\n",
    "    cicid as CIC_UniqueID,\n",
    "    gender,\n",
    "    biryear,\n",
    "    visatype,\n",
    "    occup\n",
    "From\n",
    "    i94\n",
    "\"\"\").dropDuplicates()\n",
    "\n",
    "dimTravelFlags = spark.sql(\"\"\"\n",
    "Select\n",
    "    cicid as CIC_UniqueID,\n",
    "    ENTDEPA as ArrivalFlag,\n",
    "    ENTDEPD as DepartureFlag,\n",
    "    ENTDEPU as UpdateFlag,\n",
    "    MATFLAG as MatchFlag\n",
    "From\n",
    "    i94\n",
    "\"\"\").dropDuplicates()\n",
    "\n",
    "dimAirline = spark.sql(\"\"\"\n",
    "Select\n",
    "    cicid as CIC_UniqueID,\n",
    "    arrdate,\n",
    "    AIRLINE as Airline,\n",
    "    FLTNO as FlightNumber\n",
    "From\n",
    "    i94\n",
    "\"\"\").dropDuplicates()\n",
    "\n",
    "dimTravelInfo = spark.sql(\"\"\"\n",
    "Select\n",
    "    cicid as CIC_UniqueID,\n",
    "    VISATYPE as TypeOfVisa,\n",
    "    ADMNUM as AdmissionNumber,\n",
    "    VISAPOST as VisaIssuance,\n",
    "    I94VISA as PurposeOfTravel,\n",
    "    I94MODE as ModeOfTravel\n",
    "From\n",
    "    i94\n",
    "\"\"\").dropDuplicates()\n",
    "\n",
    "dimStateInfo = spark.sql(\"\"\"\n",
    "Select\n",
    "    State,\n",
    "    StateCode,\n",
    "    Male,\n",
    "    Female,\n",
    "    Population,\n",
    "    Veterans,\n",
    "    AlienPopulation,\n",
    "    AvgHouseHoldSize,\n",
    "    AverageAge\n",
    "From\n",
    "    States\n",
    "\"\"\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DestBucket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-203be68cc7b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#DestBucket = \"s3://stagingi94/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfact_i94\u001b[0m\u001b[0;34m.\u001b[0m    \u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m    \u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDestBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fact_i94.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m     \u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i94cit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"i94res\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"i94port\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"i94addr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdimAirport\u001b[0m\u001b[0;34m.\u001b[0m    \u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m    \u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDestBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dimAirport.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m     \u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iso_region\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"municipality\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"airporttype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdimTraveller\u001b[0m\u001b[0;34m.\u001b[0m    \u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m    \u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDestBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dimTraveller.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m     \u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CIC_UniqueID\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"biryear\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"visatype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DestBucket' is not defined"
     ]
    }
   ],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "#df_spark=spark.read.parquet(\"sas_data\")\n",
    "\n",
    "# Write the data mart to S3 bucket\n",
    "#DestBucket = \"s3://stagingi94/\"\n",
    "\n",
    "fact_i94.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(DestBucket, \"fact_i94.parquet\")). \\\n",
    "    partitionBy(\"i94cit\",\"i94res\",\"i94port\",\"i94addr\")\n",
    "dimAirport.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(DestBucket, \"dimAirport.parquet\")). \\\n",
    "    partitionBy(\"iso_region\",\"municipality\",\"airporttype\")\n",
    "dimTraveller.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(DestBucket, \"dimTraveller.parquet\")). \\\n",
    "    partitionBy(\"CIC_UniqueID\",\"biryear\",\"visatype\")\n",
    "dimTravelFlags.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(DestBucket, \"dimTravelFlags.parquet\")). \\\n",
    "    partitionBy(\"CIC_UniqueID\")\n",
    "dimDate.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(DestBucket, \"dimDate.parquet\"))\n",
    "dimAirline.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(DestBucket, \"dimAirline.parquet\")). \\\n",
    "    partitionBy(\"Airline\")\n",
    "dimTravelInfo.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(DestBucket, \"dimTravelInfo.parquet\")). \\\n",
    "    partitionBy(\"CIC_UniqueID\",\"TypeOfVisa\",\"VisaIssuance\")\n",
    "dimStateInfo.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(DestBucket, \"dimStateInfo.parquet\")). \\\n",
    "    partitionBy(\"State\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "For data quality checks, first, I am looking at joining a fact and dimension table to verify the records match appropriately followed by checking for dupliation on the fact table using CICIC Id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+--------+---------+\n",
      "|CIC_UniqueID|i94port|i94addr|   State|StateCode|\n",
      "+------------+-------+-------+--------+---------+\n",
      "|        91.0|    CLT|     IN| Indiana|       IN|\n",
      "|       151.0|    NEW|     NY|New York|       NY|\n",
      "|       938.0|    NEW|     NY|New York|       NY|\n",
      "+------------+-------+-------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_i94.createOrReplaceTempView(\"dq_test\")\n",
    "dimStateInfo.createOrReplaceTempView(\"dq_test1\")\n",
    "dq_testquery = spark.sql(\"\"\"\n",
    "select \n",
    "    a.CIC_UniqueID,\n",
    "    a.i94port,\n",
    "    a.i94addr,\n",
    "    b.State,\n",
    "    b.StateCode\n",
    "From dq_test a\n",
    "Join dq_test1 b\n",
    "on a.i94addr == b.StateCode\n",
    "\"\"\")\n",
    "dq_testquery.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|CIC_UniqueID|count|\n",
      "+------------+-----+\n",
      "|     12467.0|    1|\n",
      "|     35804.0|    1|\n",
      "|     52086.0|    1|\n",
      "+------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DQ Check #2\n",
    "# look for duplications\n",
    "dq_testquery2 = spark.sql(\"\"\"\n",
    "select CIC_UniqueID, count(*) as count from dq_test group by CIC_UniqueID order by count\n",
    "\"\"\")\n",
    "dq_testquery2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "In this project, I used Spark to develop data mart and write the ETL pipeline to load cleaned data into data mart. I was pleased Spark took only few minutes to load and perform operations on dataset with 3.1 million rows in it.\n",
    "* Propose how often the data should be updated and why.\n",
    "The file structure shows the dataset is prepared every month. Perhaps if this ETL process runs every month, that would be ideal. We might want to have development environment where we run the pipeline to see if we encounter any issues before moving to production environment.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " Put together a proper Amazon EMR cluster mx.extralarge with one name node and several worker nodes (15-20)\n",
    " Utilize Apache Airflow to schedule and monitor workflows.\n",
    " Also, utilize a resource manager that would display the nodes utilization and amount of physical memory that is being consumed and adjust workload accordingly.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " Using Apache Airflow to schedule the workflow and create a email mechanism \n",
    " If the data is being refreshed every day (which does not seem to be the case since the data sets are prepared monthly) then identify data points that more often (US Travel Data) and schedule that to update every so often.\n",
    " Some data may not change every day - for instance, airport codes are never meant to change and remain static.\n",
    " * The database needed to be accessed by 100+ people.\n",
    " Enable Server Encryption to prevent DDoS attacks\n",
    " Enable replication - backups to speed up recovery\n",
    " Increase number of worker nodes in EMR and increase replication\n",
    " Let the data sit on Hadoop DFS and implement Hive to retrieve data quickly from the HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
